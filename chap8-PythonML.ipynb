{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing movie dataset into more convenient format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind \n",
    "import pandas as pd  \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = './data/aclImdb' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/Users/krish.mahajan@ibm.com/Desktop/Documents_Desktop/Github/amazon-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:03:06\n"
     ]
    }
   ],
   "source": [
    "labels = {'pos':1,'neg':0} \n",
    "pbar = pyprind.ProgBar(50000) \n",
    "df = pd.DataFrame() \n",
    "for s in ('test' , 'train') :\n",
    "    for l in ('pos' , 'neg'):\n",
    "        path = os.path.join(basepath, s , l) \n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path,file),'r',encoding='utf-8') as infile: \n",
    "                txt = infile.read() \n",
    "                df = df.append([[txt,labels[l]]],ignore_index=True) \n",
    "                pbar.update() \n",
    "df.columns = ['review','sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Based on an actual story, John Boorman shows t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a gem. As a Film Four production - the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I really like this show. It has drama, romance...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is the best 3-D experience Disney has at ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Of the Korean movies I've seen, only three had...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  Based on an actual story, John Boorman shows t...          1\n",
       "1  This is a gem. As a Film Four production - the...          1\n",
       "2  I really like this show. It has drama, romance...          1\n",
       "3  This is the best 3-D experience Disney has at ...          1\n",
       "4  Of the Korean movies I've seen, only three had...          1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>My comments may be a bit of a spoiler, for wha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The \"saucy\" misadventures of four au pairs who...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>Oh, those Italians! Assuming that movies about...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Eight academy nominations? It's beyond belief....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>Not that I dislike childrens movies, but this ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49995  My comments may be a bit of a spoiler, for wha...          0\n",
       "49996  The \"saucy\" misadventures of four au pairs who...          0\n",
       "49997  Oh, those Italians! Assuming that movies about...          0\n",
       "49998  Eight academy nominations? It's beyond belief....          0\n",
       "49999  Not that I dislike childrens movies, but this ...          0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       This is a gem. As a Film Four production - the...\n",
       "sentiment                                                    1\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling  the dataframe with permuations and creating final data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[pandas.core.indexes.range.RangeIndex,\n",
       " pandas.core.indexes.numeric.Int64Index,\n",
       " pandas.core.indexes.numeric.IntegerIndex,\n",
       " pandas.core.indexes.numeric.NumericIndex,\n",
       " pandas.core.indexes.base.Index,\n",
       " pandas.core.base.IndexOpsMixin,\n",
       " pandas.core.base.PandasObject,\n",
       " pandas.core.base.StringMixin,\n",
       " pandas.core.accessor.DirNamesMixin,\n",
       " object]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type.mro(type(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0) \n",
    "df = df.reindex(np.random.permutation(df.index)) \n",
    "df.to_csv('./data/movie_data.csv',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the final data file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/movie_data.csv' , encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I normally do not watch local mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Believe it or not, this was at one time the wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After some internet surfing, I found the \"Home...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  My family and I normally do not watch local mo...          1\n",
       "1  Believe it or not, this was at one time the wo...          0\n",
       "2  After some internet surfing, I found the \"Home...          0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data --> Word Vectors\n",
    "## 1. Bag of words model 1- gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample review \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer() \n",
    "docs = np.array(['The sun is shinning',\n",
    "                 'The weather is sweet',\n",
    "                 'The sun is shinning and weather is sweet']) \n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 5, 'sun': 3, 'is': 1, 'shinning': 2, 'weather': 6, 'sweet': 4, 'and': 0}\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 1 0]\n",
      " [0 1 0 0 1 1 1]\n",
      " [1 2 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1-gram --> \"the\" , \"sun\" , \"is\" , \"shining\"   \n",
    " ### 2-gram ---> \"the sun\" , \"sun is\" , \"is shining\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word relevancy via term frequency-inverse document frequency \n",
    "#### **tf-idf(t,d) = tf(t,d) X idf(t,d)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.43 0.56 0.56 0.   0.43 0.  ]\n",
      " [0.   0.43 0.   0.   0.56 0.43 0.56]\n",
      " [0.44 0.53 0.34 0.34 0.34 0.26 0.34]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "tfidf = TfidfTransformer(use_idf=True,norm='l2',smooth_idf=True) \n",
    "np.set_printoptions(precision=2) \n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Text Cleaning & processing text into documents & stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My family and I normally do not watch local movies for the simple reason that they are poorly made, they lack the depth, and just not worth our time.<br /><br />The trailer of \"Nasaan ka man\" caught my attention, my daughter in law\\'s and daughter\\'s so we took time out to watch it this afternoon. The movie exceeded our expectations. The cinematography was very good, the story beautiful and the acting awesome. Jericho Rosales was really very good, so\\'s Claudine Barretto. The fact that I despised Diether Ocampo proves he was effective at his role. I have never been this touched, moved and affected by a local movie before. Imagine a cynic like me dabbing my eyes at the end of the movie? Congratulations to Star Cinema!! Way to go, Jericho and Claudine!!'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0 , 'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning \n",
    "import re \n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>','',text) \n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text) \n",
    "    text = (re.sub('[\\W]+',' ',text.lower())+ ' '.join(emoticons).replace('-',''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my family and i normally do not watch local movies for the simple reason that they are poorly made they lack the depth and just not worth our time the trailer of nasaan ka man caught my attention my daughter in law s and daughter s so we took time out to watch it this afternoon the movie exceeded our expectations the cinematography was very good the story beautiful and the acting awesome jericho rosales was really very good so s claudine barretto the fact that i despised diether ocampo proves he was effective at his role i have never been this touched moved and affected by a local movie before imagine a cynic like me dabbing my eyes at the end of the movie congratulations to star cinema way to go jericho and claudine '"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaned version \n",
    "df.loc[0,'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization \n",
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "porter = PorterStemmer() \n",
    "def tokenizer_porter(text): \n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/krish.mahajan@ibm.com/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#stopwords removing \n",
    "import nltk \n",
    "nltk.download('stopwords') \n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'thu', 'run', 'lot']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in tokenizer_porter('a runner like running and thus they runs a lot') if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a logistic regression model for document classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First dividing the DataFrame of cleaned text document into 25,000 documents for training and 25000 documents for testing \n",
    "\n",
    "X_train = df.loc[:25000,'review'].values \n",
    "y_train = df.loc[:25000,'sentiment'].values \n",
    "X_test = df.loc[25000:,'review'].values\n",
    "y_test = df.loc[25000:,'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing GridSearchCV to find the optimal set of parameters for the logistic regression model using 5 fold stratified cross validation\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "/Users/krish.mahajan@ibm.com/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed: 16.6min finished\n",
      "/Users/krish.mahajan@ibm.com/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...e, penalty='l2', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid=[{'vect__ngram_range': [(1, 1)], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's...ion tokenizer_porter at 0x1a1f493950>], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None,lowercase=False,preprocessor=None) \n",
    "param_grid = [{'vect__ngram_range':[(1,1)],\n",
    "               'vect__stop_words' :[stop,None],\n",
    "            'vect__tokenizer':[tokenizer,tokenizer_porter],\n",
    "              'clf__penalty' :['l1','l2'],\n",
    "              'clf__C':[1.0, 10.0,100.0]}] \n",
    "lr_tfidf = Pipeline([('vect',tfidf),\n",
    "                     ('clf' ,LogisticRegression(random_state=0))]) \n",
    "\n",
    "gs_lr_tfidf= GridSearchCV(lr_tfidf,param_grid,scoring='accuracy',cv=2,verbose=1,n_jobs=-1)\n",
    "gs_lr_tfidf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x1a1f493c80>} \n"
     ]
    }
   ],
   "source": [
    "print('Best Parameter set: %s ' % gs_lr_tfidf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.886\n"
     ]
    }
   ],
   "source": [
    "print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.900\n"
     ]
    }
   ],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with bigger data - online algorithms and out-of-core learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import re \n",
    "from nltk.corpus import stopwords \n",
    "stop = stopwords.words('english') \n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>','',text) \n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text) \n",
    "    text = (re.sub('[\\W]+',' ',text.lower())+ ' '.join(emoticons).replace('-','')) \n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path,'r' , encoding='utf-8') as csv:\n",
    "        next(csv) # skip header \n",
    "        for line in csv:\n",
    "            text , label = line[:-3] , int(line[-2]) \n",
    "            yield text,label \n",
    "            \n",
    "def get_minitbatch(doc_stream,size):\n",
    "    docs , y = [] , [] \n",
    "    try:\n",
    "        for _ in range(size) :\n",
    "            text , label = next(doc_stream) \n",
    "            docs.append(text) \n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None,None \n",
    "    return docs,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer \n",
    "from sklearn.linear_model import SGDClassifier \n",
    "vect = HashingVectorizer(decode_error='ignore',n_features=2**21,preprocessor=None,tokenizer=tokenizer)\n",
    "clf = SGDClassifier(loss='log',random_state=1,n_iter=1)\n",
    "doc_stream = stream_docs(path='./data/movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:23\n"
     ]
    }
   ],
   "source": [
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0,1]) \n",
    "for _ in range(45):\n",
    "    X_train,y_train = get_minitbatch(doc_stream,size=1000) \n",
    "    if not X_train:\n",
    "        break \n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train,y_train,classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.868\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minitbatch(doc_stream,size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy : %.3f' % clf.score(X_test,y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serializing fitted scikit-learn estimators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import os\n",
    "dest = os.path.join('movieclassifier','pkl_objets') \n",
    "\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest) \n",
    "    \n",
    "#Serializing stop words\n",
    "pickle.dump(stop,open(os.path.join(dest,'stopwords.pkl'),'wb'),protocol=4)  \n",
    "\n",
    "#Serializing classifier model\n",
    "pickle.dump(clf,open(os.path.join(dest,'classifier.pkl'),'wb'),protocol=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with Latent Dirichlet Allocation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('./data/movie_data.csv', encoding='utf-8') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "count = CountVectorizer(stop_words='english',max_df=.1,max_features=5000)\n",
    "X = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation \n",
    "lda = LatentDirichletAllocation(n_topics=10,random_state=123,learning_method='batch')\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "worst minutes awful script stupid\n",
      "Topic 2:\n",
      "family mother father girl children\n",
      "Topic 3:\n",
      "american dvd war music tv\n",
      "Topic 4:\n",
      "human audience cinema art feel\n",
      "Topic 5:\n",
      "police guy car dead murder\n",
      "Topic 6:\n",
      "horror house gore blood sex\n",
      "Topic 7:\n",
      "role performance comedy actor performances\n",
      "Topic 8:\n",
      "series episode war episodes season\n",
      "Topic 9:\n",
      "book version original effects read\n",
      "Topic 10:\n",
      "action fight guy guys cool\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names() \n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" %(topic_idx + 1))\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words -1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Horror Movie #1:\n",
      "Emilio Miraglia's first Giallo feature, The Night Evelyn Came Out of the Grave, was a great combination of Giallo and Gothic horror - and this second film is even better! We've got more of the Giallo side of the equation this time around, although Miraglia doesn't lose the Gothic horror stylings tha ...\n",
      "\n",
      "Horror Movie #2:\n",
      "House of Dracula works from the same basic premise as House of Frankenstein from the year before; namely that Universal's three most famous monsters; Dracula, Frankenstein's Monster and The Wolf Man are appearing in the movie together. Naturally, the film is rather messy therefore, but the fact that ...\n",
      "\n",
      "Horror Movie #3:\n",
      "This film marked the end of the \"serious\" Universal Monsters era (Abbott and Costello meet up with the monsters later in \"Abbott and Costello Meet Frankentstein\"). It was a somewhat desparate, yet fun attempt to revive the classic monsters of the Wolf Man, Frankenstein's monster, and Dracula one \"la ...\n"
     ]
    }
   ],
   "source": [
    "horror = X_topics[:,5].argsort()[::-1]\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print('\\nHorror Movie #%d:' %(iter_idx +1)) \n",
    "    print(df['review'][movie_idx][:300],'...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
